{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH10098: Numerical Linear Algebra - Workshop week 3\n",
    "\n",
    "Please note that on Notable, computational times might be a bit erratic. For comparing computational times, it is best to run the notebook on a desktop or laptop using e.g. Anaconda.\n",
    "\n",
    "### Exercise 1: Errors and Residuals (Easy)\n",
    "\n",
    "In the code cell below, set a variable `n` with value $n=50$. Use `np.random.rand` to define `A` to be a random matrix $A$ of dimension $n$. Then:\n",
    "\n",
    "(i) Create a variable `xsol` to represent a vector $x^{\\ast} \\in \\mathbb{R}^n$, all of whose entries are $1$.\n",
    "\n",
    "(ii) Compute `b = A @ xsol` to represent the vector $b=Ax^{\\ast}$.\n",
    "\n",
    "(iii) Use `np.linalg.solve` to compute `x`, the *computed* solution of $Ax=b$.\n",
    "\n",
    "(iv) Compute the $1$, $2$, and $\\infty$ norms of the residual vector `r = A @ x - b`. This can be done using the function `np.linalg.norm`. Note that `np.inf` is used to denote $\\infty$.\n",
    "\n",
    "(v) Compute the $1$, $2$, and $\\infty$ norms of the solution error `e = x - xsol`.\n",
    "\n",
    "(vi) Both the residual `r` and the error `e` measure how close the computed solution `x` is to the exact solution `xsol`. Report the norms of the residual and solution error. How do the norms of `r` and `e` compare?\n",
    "\n",
    "Now repeat parts (i-vi) with `n = 50` and `A = hilbert(n)` (using the function `hilbert` from SciPy's `scipy.linalg` module), where `A` represents the Hilbert matrix $A$ of dimension $n$. Do you notice a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import hilbert\n",
    "\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import hilbert\n",
    "\n",
    "n = 50\n",
    "A = np.random.rand(n, n)\n",
    "# A = hilbert(n)\n",
    "\n",
    "xsol = np.ones(n)\n",
    "b = A @ xsol\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "r = A @ x - b\n",
    "e = x - xsol\n",
    "\n",
    "for p in [1, 2, np.inf]:\n",
    "    r_norm = np.linalg.norm(r, p)\n",
    "    e_norm = np.linalg.norm(e, p)\n",
    "    print('{}-norm:\\n'\n",
    "         'r : {:.6g}\\n'\n",
    "         'e : {:.6g}\\n'.format(p, r_norm, e_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Comments:*** For random matrices, the norms of both the residual and the error are small, typically in the order of $10^{-13}$ or $10^{-14}$. The norm of the residual is usually slightly smaller than the norm of the error, by a factor of about $2$ to $10$.\n",
    "\n",
    "For the Hilbert matrix, we see that the residual is small (around $10^{-14}$), but the error is large (around $10^2$). We already saw in the computer lab in week 1 that computations with Hilbert matrices are very sensitive to rounding errors, and that small rounding errors can lead to very inaccurate results. The residual only involves the matrix $A$, but the error involves the matrix $A^{-1}$, which is why it is so large. (We will learn more about this in the lectures.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: LU factorisation (Standard)\n",
    "\n",
    "In the code cell below:\n",
    "\n",
    "(i) Write a Python function `LU` to implement Algorithm LU from the lecture notes. As stated in the algorithm, your function should take a single input variable `A`, representing a matrix $A \\in \\mathbb{R}^{n\\times n}$, and return two variables, `L` and `U`, representing the matrices $L, U \\in \\mathbb{R}^{n\\times n}$ in the LU factorisation of $A$, such that $A = LU$.\n",
    "\n",
    "(ii) Test your function on the following example:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "2&1&1 \\\\ 4&3&3 \\\\ 8&7&9\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1&0&0 \\\\ 2&1&0 \\\\ 4&3&1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2&1&1 \\\\ 0&1&1 \\\\ 0&0&2\n",
    "\\end{bmatrix}\n",
    "= LU.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LU(A):\n",
    "    # add code here\n",
    "    \n",
    "    return L, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LU(A):\n",
    "    # Find dimension of A\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Initialise L, U\n",
    "    L = np.eye(n)\n",
    "    U = np.copy(A)\n",
    "\n",
    "    for k in range(n - 1):\n",
    "        for j in range(k + 1, n):\n",
    "            L[j, k] = U[j, k] / U[k ,k]\n",
    "            U[j, k:] = U[j, k:] - L[j, k] * U[k, k:]\n",
    "    \n",
    "    return L, U\n",
    "\n",
    "A = np.array([[2, 1, 1], [4, 3, 3], [8, 7, 9]], dtype=float)\n",
    "L, U = LU(A)\n",
    "print(L)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Substitution\n",
    "\n",
    "The function `BS` below solves the equation $Ux=y$, for a non-singular, upper triangular matrix $U\\in \\mathbb{R}^{n\\times n}$ and a vector $y \\in \\mathbb{R}^n$, using Algorithm BS from the lecture notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def BS(U, y):\n",
    "    n = U.shape[0]\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    for j in range(n-1, -1, -1):\n",
    "        x[j] = (y[j] - U[j, j+1:] @ x[j+1:]) / U[j, j]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Comments:*** Note that for `j = n - 1`, the vectors `U[j, j+1:]` and `x[j+1:]` are empty. This corresponds to the fact that the sum in Line 2 of Algorithm BS does not contain any terms for $j=n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Forward Substitution (Standard)\n",
    "\n",
    "(i) Write down (in pseudocode) the forward substitution algorithm, i.e. write an algorithm that solves the equation $Ly=b$, for a non-singular, lower triangular matrix $L\\in \\mathbb{R}^{n\\times n}$ and a vector $b \\in \\mathbb{R}^n$. (This should be similar to Algorithm BS from the lecture notes.)\n",
    "\n",
    "(ii) What is the computational cost of the forward substitution algorithm?\n",
    "\n",
    "(iii) In the cell below, write a function `FS` to implement your forward substitution algorithm. Your function should take two input arguments, `L` and `b`, and return one output variable, `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def FS(L, b):\n",
    "    n = L.shape[0]\n",
    "    y = np.zeros(n)\n",
    "    \n",
    "    for j in range(n):\n",
    "        y[j] = (b[j] - L[j, :j] @ y[:j]) / L[j, j]\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Comments:*** Note that for `j = 0`, the vectors `L[j, :j]` and `y[:j]` are empty. This corresponds to the fact that the sum inside the loop does not contain any terms for `j = 0`.\n",
    "\n",
    "The line inside the loop requires $j-1$ multiplications, $j-2$ additions, 1 subtraction and 1 division. Summing over the loop iterations, we have\n",
    "\n",
    "$$\n",
    "C(n) = \\sum_{j=1}^n (2(j-1)+1)\n",
    "= 2 \\sum_{l=0}^{n-1} l + n\n",
    "= n(n-1) + n = n^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Gaussian Elimination (Easy)\n",
    "\n",
    "(i) In the code cell below, write a function `GE` which performs Gaussian elimination, using your functions `LU`, `FS`, and `BS`. The function `GE` should take two input arguments, `A` and `b`, and return one variable `x`, representing the solution $x \\in \\mathbb{R}^n$ of $Ax=b$.\n",
    "\n",
    "*Note: recall that you do not need to copy/paste your previous functions into the cell below -- simply run the cells in which you have written the functions to store them in memory and make them available to* `GE`.\n",
    "\n",
    "(ii) Test your function `GE` with the following values:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "2&1&1 \\\\ 4&3&3 \\\\ 8&7&9\n",
    "\\end{bmatrix}\\, , \\quad\n",
    "b =\n",
    "\\begin{bmatrix}\n",
    "4 \\\\ 10 \\\\ 24\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "(iii) Determine the elapsed time and time ratio when using your function `GE` to solve $Ax=b$ using a random matrix $A$ and random vector $b$ of dimension $n = 50 \\times 2^k$ for $k=1, 2, \\dots, 5$. Are the results as you expected?\n",
    "\n",
    "(Please note that on Notable, the computational times might be a bit erratic. For comparing computational times, it is best to run the notebook on a desktop or laptop using e.g. Anaconda.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def GE(A, b):\n",
    "    L, U = LU(A)\n",
    "    y = FS(L, b)\n",
    "    x = BS(U, y)\n",
    "    return x\n",
    "\n",
    "# Testing the function GE\n",
    "A = np.array([[2, 1, 1], [4, 3, 3], [8, 7, 9]], dtype=float)\n",
    "b = np.array([4, 10, 24], dtype=float)\n",
    "x = GE(A, b)\n",
    "print('x = {}'.format(x))\n",
    "\n",
    "# Timing with random inputs\n",
    "t = []\n",
    "for k in range(1, 6):\n",
    "    n = 50 * 2**k\n",
    "    A = np.random.rand(n, n)\n",
    "    b = np.random.rand(n)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    x = GE(A, b)\n",
    "    t1 = time.time() - t0\n",
    "    t.append(t1)\n",
    "    \n",
    "    print('Dimension n = {}\\nComputation time: {:.5g} s\\n'.format(n, t1))\n",
    "\n",
    "# Time ratios\n",
    "t_ratio = [t[i+1] / t[i] for i in range(4)]\n",
    "print('Time ratios:\\n{}'.format(t_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Comments:*** The computational cost of Gaussian elimination grows with $O(n^3)$, so for large values of $n$, we expect the ratio of times for $n$ and $2n$ to be $8$. The values of $n$ here are still quite small, and we tupically observe ratios of around 5-6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Efficiency of Gaussian Elimination (Easy)\n",
    "\n",
    "In Algorithm LU, consider replacing Line 5 by\n",
    "\n",
    "$$\n",
    "5': \\quad\n",
    "(u_{j1}, \\dots, u_{jn}) = (u_{j1}, \\dots, u_{jn}) - l_{jk}(u_{k1}, \\dots, u_{kn})\n",
    "$$\n",
    "\n",
    "(i) Explain why this change does not affect the computed matrices $L$ and $U$.\n",
    "\n",
    "(ii) Explain why this change does affect the computational cost of Algorithm LU. What is the cost of the modified algorithm?\n",
    "\n",
    "(iii) Modify your code above to implement this modification. Do you see a difference in the computational times to solve a system $Ax=b$, between Algorithm GE and the modified version with the LU factorisation modified as above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution:***\n",
    "\n",
    "(i) This change does not affect the computed results, since the first $k-1$ columns of $U$ contain zeros below the diagonal after iteration $k-1$. In the first $k-1$ columns, the operations in Line $5'$ are simple subtractions of zeros.\n",
    "\n",
    "(ii) This change does influence the computational cost, since we are adding additional (and unnecessary) computations.\n",
    "\n",
    "Counting the FLOPs: Line $5'$ requires $n$ multiplications and $n$ subtractions. Line $4$ requires $1$ division. Summing over the loops, we have\n",
    "\n",
    "\\begin{align}\n",
    "C(n) &= \\sum_{k=1}^{n-1} \\sum_{j=k+1}^n (2n+1) \\\\\n",
    "&= \\sum_{k=1}^{n-1} (2n+1)(n-k) \\\\\n",
    "&= (2n+1) \\sum_{k=1}^{n-1} (n-k) \\\\\n",
    "&=  (2n+1) \\sum_{l=1}^{n-1} l \\\\\n",
    "&= (2n+1)\\frac{n(n-1)}{2} \\\\\n",
    "&= n^3 -\\frac{n^2}{2} - \\frac{n}{2}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Block LU factorisation (Difficult)\n",
    "\n",
    "A *divide-and-conquer* method aims to solve a large problem *recursively*. The main problem is subdivided (*branched*) into 2 smaller problems, and these problems are each themselves subdivided -- until we end up with many simpler problems, each solvable by a direct method. The solution to the main problem is finally obtained by combining the solutions of the small problems.\n",
    "\n",
    "For example, the following function computes $x^n, x \\in \\mathbb{R}, n \\in \\mathbb{N}$ relatively efficiently using a divide-and-conquer approach, by further taking advantage of the problem symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "def power(x, n):\n",
    "    '''\n",
    "    Compute x^n using a divide-and-conquer method.\n",
    "    x^n is written as x^n = x^(2m + k), where k may be 0 or 1,\n",
    "    which allows to only have to compute 1 branch at every depth level.\n",
    "    '''\n",
    "    # Reached the last level\n",
    "    if n == 1:\n",
    "        return x\n",
    "    \n",
    "    # Write x^n = x^(2m + k), where k = 0 or 1\n",
    "    m = n // 2\n",
    "    k = n - 2*m\n",
    "    \n",
    "    # Compute x^m recursively\n",
    "    xm = power(x, m)\n",
    "    \n",
    "    # Separate even and odd cases\n",
    "    if k == 0:\n",
    "        return xm * xm\n",
    "    else:\n",
    "        return x * xm * xm\n",
    "    \n",
    "print(power(2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A \\in \\mathbb{R}^{n\\times n}$, where $n = 2^k$ for some $k \\in \\mathbb{N}$, be a non-singular matrix. The LU factorisation of $A$ can be written in block form as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A_{11} & A_{12} \\\\\n",
    "A_{21} & A_{22}\n",
    "\\end{bmatrix}\n",
    "= A = LU =\n",
    "\\begin{bmatrix}\n",
    "L_{11} & 0 \\\\\n",
    "L_{21} & L_{22}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "U_{11} & U_{12} \\\\\n",
    "0 & U_{22}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where all the blocks are of size $\\frac{n}{2} \\times \\frac{n}{2}$.\n",
    "\n",
    "(i) Devise an algorithm DAC-LU, which uses a divide-and-conquer strategy for LU factorisation.\n",
    "\n",
    "(ii) Write a function `DAC_LU` which computes the LU decomposition of a matrix $A$ using your algorithm.\n",
    "\n",
    "(iii) For $n = 2^k$, with $k=6, \\dots, 11$, generate a random matrix $A\\in \\mathbb{R}^{n\\times n}$, and compute its LU decomposition using both `LU` and `DAC_LU` functions. Report the computation times obtained with both methods. What do you observe?\n",
    "\n",
    "(iv) What is the cost of Algorithm DAC-LU?\n",
    "\n",
    "Some tips to get you started:\n",
    "\n",
    "* You may wish to modify your `FS` and/or `BS` functions in order to solve systems of the form $LY=B$ and/or $UX=Y$, respectively, where $X, Y, B \\in \\mathbb{R}^{n \\times m}$.\n",
    "* $XA=B \\Leftrightarrow (XA)^T = B^T$.\n",
    "* The function [`numpy.allclose`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html) may be useful to check that `DAC_LU` is computing the correct matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution:***\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A_{11} & A_{12} \\\\\n",
    "A_{21} & A_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "L_{11} U_{11} & L_{11} U_{12} \\\\\n",
    "L_{21} U_{11} & L_{21} U_{12} + L_{22} U_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$L_{11}, L_{22}$ are unit lower triangular, and $U_{11}, U_{22}$ are upper triangular, therefore\n",
    "* $A_{11} = L_{11} U_{11}$ is the LU factorisation of $A_{11}$, and\n",
    "* $A_{22} - L_{21} U_{12} = L_{22} U_{22}$ is the LU factorisation of $A_{22} - L_{21} U_{12}$.\n",
    "\n",
    "**Algorithm DAC-LU:** LU factorisation by divide-and-conquer approach.\n",
    "\n",
    "**Input:** $A \\in \\mathbb{R}^{n\\times n}$ non-singular\n",
    "\n",
    "**Output:** $L, U \\in \\mathbb{R}^{n\\times n}$, with $L$ unit lower triangular, $U$ non-singular upper triangular, such that $A = LU$\n",
    "\n",
    "1. Compute $L_{11} U_{11} = A_{11}$, the LU factorisation of $A_{11}$, recursively\n",
    "2. Solve $L_{11} U_{12} = A_{12}$ for $U_{12}$ using Algorithm FS, $\\frac{n}{2}$ times\n",
    "3. Solve $\\left(L_{21} U_{11}\\right)^T = \\left(A_{21}\\right)^T$ for $L_{21}$ using Algorithm FS, $\\frac{n}{2}$ times\n",
    "4. Compute $L_{22} U_{22} = S$, the LU factorisation of $S = A_{22} - L_{21} U_{12}$, recursively\n",
    "5. Assemble $L$, $U$ from their respective 4 blocks\n",
    "\n",
    "*Note:* $S = A_{22} - L_{21} U_{12} = A_{22} - A_{21} A_{11}^{-1} A_{12}$ is the Schur complement of $A_{11}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def FS(L, B):\n",
    "    '''\n",
    "    Rewrite FS to handle LY = B.\n",
    "    '''\n",
    "    # N is a tuple indicating the dimensions of B\n",
    "    N = B.shape\n",
    "    \n",
    "    # Initialise Y the same shape as B\n",
    "    Y = np.zeros(N)\n",
    "    \n",
    "    # Scalar case - the same as before\n",
    "    if len(N) == 1:\n",
    "        for j in range(N[0]):\n",
    "            Y[j] = (B[j] - L[j, :j] @ Y[:j]) / L[j, j]\n",
    "            \n",
    "    # Vector case - solve all systems at once (faster than looping!)\n",
    "    else:\n",
    "        for j in range(N[0]):\n",
    "            Y[j, :] = (B[j, :] - L[j, :j] @ Y[:j, :]) / L[j, j]\n",
    "        \n",
    "    return Y\n",
    "\n",
    "\n",
    "def DAC_LU(A):\n",
    "    '''\n",
    "    LU factorisation by a divide-and-conquer method.\n",
    "    '''\n",
    "    n = A.shape[0]\n",
    "    m = n // 2\n",
    "    \n",
    "    # End recursion\n",
    "    if m == 1:\n",
    "        L21 = A[1, 0] / A[0, 0]\n",
    "        U22 = A[1, 1] - L21 * A[0, 1]\n",
    "        L = np.array([[1, 0], [L21, 1]])\n",
    "        U = np.array([[A[0, 0], A[0, 1]], [0, U22]])\n",
    "        return L, U\n",
    "    \n",
    "    # Compute LU decomposition of A11\n",
    "    L11, U11 = DAC_LU(A[:m, :m])\n",
    "    \n",
    "    # Solve L11 U12 = A12\n",
    "    U12 = FS(L11, A[:m, m:])\n",
    "    \n",
    "    # Solve L21 U11 = A21\n",
    "    L21 = FS(U11.T, A[m:, :m].T).T\n",
    "    \n",
    "    # Compute LU decomposition of S = A22 - L21 U12\n",
    "    L22, U22 = DAC_LU(A[m:, m:] - L21 @ U12)\n",
    "    \n",
    "    # Assemble L, U\n",
    "    L = np.block([[L11, np.zeros((m, m))], [L21, L22]])\n",
    "    U = np.block([[U11, U12], [np.zeros((m, m)), U22]])\n",
    "    \n",
    "    return L, U\n",
    "\n",
    "\n",
    "# Testing\n",
    "t_LU = []\n",
    "t_DAC_LU = []\n",
    "n_vals = []\n",
    "\n",
    "for i in range(6):\n",
    "    # Set up system\n",
    "    k = i + 6\n",
    "    n = 2**k\n",
    "    n_vals.append(n)\n",
    "    \n",
    "    A = np.random.rand(n, n)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    L, U = LU(A)\n",
    "    t_LU.append(time.time() - t0)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    L_DAC, U_DAC = DAC_LU(A)\n",
    "    t_DAC_LU.append(time.time() - t0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(n_vals, t_LU, 'rx', label='LU')\n",
    "ax.plot(n_vals, t_DAC_LU, 'bo', label='DAC_LU')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Comments:*** Using Algorithm DAC-LU seems to become advantageous for larger values of $n$.\n",
    "\n",
    "The cost of each step Algorithm DAC-LU is:\n",
    "1. $\\mathcal{C}_{\\text{DAC-LU}}(\\frac{n}{2})$\n",
    "2. $\\frac{n}{2} \\mathcal{C}_{\\text{FS}}(\\frac{n}{2})$\n",
    "3. $\\frac{n}{2} \\mathcal{C}_{\\text{FS}}(\\frac{n}{2})$\n",
    "4. $\\mathcal{C}_S$ (the cost of computing $S$, i.e. matrix multiplication + subtraction), and $\\mathcal{C}_{\\text{DAC-LU}}(\\frac{n}{2})$\n",
    "\n",
    "The total cost $\\mathcal{C}_{\\text{DAC-LU}}(n)$ is therefore\n",
    "$$\n",
    "\\mathcal{C}_{\\text{DAC-LU}}(n) = 2 \\mathcal{C}_{\\text{DAC-LU}}(\\frac{n}{2})\n",
    "+ n \\mathcal{C}_{\\text{FS}}(\\frac{n}{2})\n",
    "+ \\mathcal{C}_S\n",
    ", \\quad \\text{with } \\mathcal{C}_{\\text{DAC-LU}}(1) = 1.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
